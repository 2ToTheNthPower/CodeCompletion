{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This Notebook demonstrates a N-gram Programming Model that is built upon an existing N-Gram Sentence Completion Model.\n",
        "\n",
        "The goal of developing this model is to see how does a simple model perform in comparison to the complex Neural Networks and Deep-Learning Models."
      ],
      "metadata": {
        "id": "gMoz3xynGvMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The N-gram Sentence Completion model that was used has been cited.\n",
        "\n",
        "References: https://www.kaggle.com/code/sauravmaheshkar/auto-completion-using-n-gram-models/notebook"
      ],
      "metadata": {
        "id": "NvEYxOqKHBNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eCgEKw9pHNcT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9wahnUuL6qT",
        "outputId": "05e8ef3e-36d1-4c77-d960-dff278522fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Igk681bjjf1v"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "## Importing Packages\n",
        "import math\n",
        "import nltk\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## nltk settings\n",
        "nltk.download('punkt')\n",
        "\n",
        "# file_path = \"drive/MyDrive/N-Gram/tokenizer.json\"\n",
        "\n",
        "# with open(file_path, \"r\") as f:\n",
        "#   data = f.read()\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test dataset that has been used can be downloaded from here: \n",
        "\n",
        "Dataset: https://drive.google.com/file/d/1KaPCSWHMyedKq4F-90ZwpJTxmp9B4hDM/view?usp=sharing"
      ],
      "metadata": {
        "id": "Y1-HUKoKHbRW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PAfy88B5IgY"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"drive/MyDrive/N-Gram/test.csv\")\n",
        "data_str = '<EOS>'.join(data['sample'].tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "tTlTvSkHX_zM",
        "outputId": "33f79a38-8de0-4e2d-d8e9-0454c198305e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'n = int ( input ( ) ) \\n s = input ( ) \\n ans = 0 \\n ss = s [ 0 ] \\n for i in range ( 1 , n ) : \\n \\t if ss ! = s [ i ] : \\n \\t \\t ans + = 1 \\n \\t \\t ss = s [ i ] \\n print ( ans + 1 )<EOS>n = int ( input ( ) ) \\n a = list ( map ( int , input ( ) . split ( ) ) ) \\n \\n p = 1 0 0 0 \\n s = 0 \\n \\n for i , j in zip ( a , a [ 1 : ] ) : \\n if i < j : \\n d , m = divmod ( p , i ) \\n s + = d \\n p = m \\n elif i > j : \\n p + = s * i \\n s = 0 \\n \\n if s ! = 0 : \\n p + = s * a [ - 1 ] \\n \\n print ( p )<EOS>a = int ( input ( ) ) \\n b = int ( input ( ) ) \\n c = int ( input ( ) ) \\n v = int ( input ( ) ) \\n \\n ans = 0 \\n for i in range ( a + 1 ) : \\n for j in range ( b + 1 ) : \\n for k in range ( c + 1 ) : \\n if 5 0 0 * i + 1 0 0 * j + 5 0 * k = = v : \\n ans + = 1 \\n \\n print ( ans ) \\n<EOS>def framod ( n , mod , a = 1 ) : \\n for i in range ( 1 , n + 1 ) : \\n a = a * i % mod \\n return a \\n \\n def power ( n , r , mod ) : \\n if r = = 0 : return 1 \\n if r % 2 = = 0 : \\n return power ( n * n % mod , r / / 2 , mod ) % mod \\n if r % 2 = = 1 : \\n return n * powe'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_str[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cKezc2jnluRh"
      },
      "outputs": [],
      "source": [
        "def preprocess_pipeline(data) -> 'list':\n",
        "\n",
        "    # Split by newline character\n",
        "\n",
        "### WHY SPLIT DATA INTO SENTENCES?\n",
        "\n",
        "    sentences = data.split('<EOS>')\n",
        "    \n",
        "    # Remove leading and trailing spaces\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    \n",
        "    # Drop Empty Sentences\n",
        "    sentences = [s for s in sentences if len(s) > 0]\n",
        "    \n",
        "    # Empty List to hold Tokenized Sentences\n",
        "    tokenized = []\n",
        "    \n",
        "    # Iterate through sentences\n",
        "    for sentence in sentences:\n",
        "        \n",
        "        # Convert to lowercase\n",
        "        sentence = sentence.lower()\n",
        "        \n",
        "        # Convert to a list of words\n",
        "        token = nltk.word_tokenize(sentence)\n",
        "        \n",
        "        # Append to list\n",
        "        tokenized.append(token)\n",
        "        \n",
        "    return tokenized\n",
        "\n",
        "\n",
        "## Pass our data to this function    \n",
        "tokenized_sentences = preprocess_pipeline(data_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h_mdAjV6KPf"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BZwVEGE8Q5X"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(tokenized_sentences, test_size=0.2, random_state=42)\n",
        "\n",
        "train, val = train_test_split(train, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvYGa98j8zbR"
      },
      "outputs": [],
      "source": [
        "def counnt_the_words(sentences) -> 'dict':\n",
        "  word_counts = {}\n",
        "\n",
        "  for sentence in sentences:\n",
        "    for token in sentence:\n",
        "      if token not in word_counts.keys():\n",
        "        word_counts[token] = 1\n",
        "      else:\n",
        "        word_counts[token] += 1\n",
        "  return word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcUhajre9TVa"
      },
      "outputs": [],
      "source": [
        "def handling_oov(tokenized_sentences, count_threshold):\n",
        "\n",
        "  closed_vocabulary = []\n",
        "\n",
        "  words_count = counnt_the_words(tokenized_sentences)\n",
        "\n",
        "  for word, count in words_count.items():\n",
        "    if count >= count_threshold:\n",
        "      closed_vocabulary.append(word)\n",
        "  \n",
        "  return closed_vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ub7PNFx9-Ma"
      },
      "outputs": [],
      "source": [
        "def unk_tokens(tokenized_sentences, vocabulary, unknown_token = \"<unk>\"):\n",
        "  vocabulary = set(vocabulary)\n",
        "\n",
        "  new_tokenized_sentences = []\n",
        "\n",
        "  for sentence in tokenized_sentences:\n",
        "    new_sentence = []\n",
        "    for token in sentence:\n",
        "      if token in vocabulary:\n",
        "        new_sentence.append(token)\n",
        "      else:\n",
        "        new_sentence.append(unknown_token)\n",
        "  \n",
        "    new_tokenized_sentences.append(new_sentence)\n",
        "\n",
        "  return new_tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IevcxI6-AkI"
      },
      "outputs": [],
      "source": [
        "def cleansing(train_data, test_data, count_threshold):\n",
        "\n",
        "  vocabulary = handling_oov(train_data, count_threshold)\n",
        "\n",
        "  new_train_data = unk_tokens(train_data, vocabulary)\n",
        "\n",
        "  new_test_data = unk_tokens(test_data, vocabulary)\n",
        "\n",
        "  return new_train_data, new_test_data, vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0TLSJDS_Ti_"
      },
      "outputs": [],
      "source": [
        "min_freq = 6\n",
        "final_train, final_test, vocabulary = cleansing(train, test, min_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncAhuvmz_boh"
      },
      "outputs": [],
      "source": [
        "final_train[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RusLbj6RANAg"
      },
      "outputs": [],
      "source": [
        "def count_n_grams(data, n, start_token = \"<s>\", end_token = \"<e>\") -> 'dict':\n",
        "\n",
        "  # Empty dict for n-grams\n",
        "  n_grams = {}\n",
        " \n",
        "  # Iterate over all sentences in the dataset\n",
        "  for sentence in data:\n",
        "        \n",
        "    # Append n start tokens and a single end token to the sentence\n",
        "    \n",
        "### WHY MULTIPLY START_TOKENS BY N?\n",
        "\n",
        "    sentence = [start_token]*n + sentence + [end_token]\n",
        "    \n",
        "    # Convert the sentence into a tuple\n",
        "    sentence = tuple(sentence)\n",
        "\n",
        "    # Temp var to store length from start of n-gram to end\n",
        "    m = len(sentence) if n==1 else len(sentence)-1\n",
        "    \n",
        "    # Iterate over this length\n",
        "    for i in range(m):\n",
        "        \n",
        "      # Get the n-gram\n",
        "      n_gram = sentence[i:i+n]\n",
        "    \n",
        "      # Add the count of n-gram as value to our dictionary\n",
        "      # IF n-gram is already present\n",
        "      if n_gram in n_grams.keys():\n",
        "        n_grams[n_gram] += 1\n",
        "      # Add n-gram count\n",
        "      else:\n",
        "        n_grams[n_gram] = 1\n",
        "        \n",
        "  return n_grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTSrJxOnENbC"
      },
      "outputs": [],
      "source": [
        "def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k) -> 'float':\n",
        "\n",
        "  # Convert the previous_n_gram into a tuple \n",
        "  previous_n_gram = tuple(previous_n_gram)\n",
        "    \n",
        "  # Calculating the count, if exists from our freq dictionary otherwise zero\n",
        "  previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
        "  \n",
        "  # The Denominator\n",
        "  denom = previous_n_gram_count + k * vocabulary_size\n",
        "\n",
        "  # previous n-gram plus the current word as a tuple\n",
        "  nplus1_gram = previous_n_gram + (word,)\n",
        "\n",
        "  # Calculating the nplus1 count, if exists from our freq dictionary otherwise zero \n",
        "  nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n",
        "\n",
        "  # Numerator\n",
        "  num = nplus1_gram_count + k\n",
        "\n",
        "  # Final Fraction\n",
        "  prob = num / denom\n",
        "  return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8w7sEQgKEQ3P"
      },
      "outputs": [],
      "source": [
        "def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k) -> 'dict':\n",
        "\n",
        "  # Convert to Tuple\n",
        "  previous_n_gram = tuple(previous_n_gram)\n",
        "\n",
        "  # Add end and unknown tokens to the vocabulary\n",
        "\n",
        "# WHY NOT ADD START TOKENS TOO?\n",
        "\n",
        "  # Calculate the size of the vocabulary\n",
        "  vocabulary_size = len(vocabulary)\n",
        "\n",
        "  # Empty dict for probabilites\n",
        "  probabilities = {}\n",
        "\n",
        "  # Iterate over words \n",
        "  for word in vocabulary:\n",
        "    \n",
        "    # Calculate probability\n",
        "    probability = prob_for_single_word(word, previous_n_gram, \n",
        "                                           n_gram_counts, nplus1_gram_counts, \n",
        "                                           vocabulary_size, k=k)\n",
        "    # Create mapping: word -> probability\n",
        "    probabilities[word] = probability\n",
        "\n",
        "  return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A9IAs8FETiz"
      },
      "outputs": [],
      "source": [
        "def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k, num_suggestions, start_with=None):\n",
        "\n",
        "    # length of previous words\n",
        "    n = len(list(n_gram_counts.keys())[0]) \n",
        "\n",
        "    # most recent 'n' words\n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "    \n",
        "    # Calculate probabilty for all words\n",
        "    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n",
        "\n",
        "    # Intialize the suggestion and max probability\n",
        "    suggestion = []\n",
        "    max_prob = 0\n",
        "\n",
        "    # Iterate over all words and probabilites, returning the max.\n",
        "    # We also add a check if the start_with parameter is provided\n",
        "    for word, prob in probabilities.items():\n",
        "        \n",
        "        # if start_with != None: \n",
        "            \n",
        "        #     if not word.startswith(start_with):\n",
        "        #         continue \n",
        "\n",
        "        if prob > max_prob: \n",
        "          if len(suggestion) < num_suggestions:\n",
        "            suggestion.append((word, prob))\n",
        "          else:\n",
        "            flag = False\n",
        "            idx = 0\n",
        "            while flag == False and idx < len(suggestion):\n",
        "              if suggestion[idx][1] < prob:\n",
        "                flag = True\n",
        "                suggestion[idx] = (word, prob)\n",
        "              idx += 1\n",
        "\n",
        "    return suggestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FziGYkiEWbN"
      },
      "outputs": [],
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k, start_with=None, num_suggestions=5):\n",
        "\n",
        "    # See how many models we have\n",
        "    count = len(n_gram_counts_list)\n",
        "    \n",
        "    # Empty list for suggestions\n",
        "    suggestions = []\n",
        "    \n",
        "    # IMP: Earlier \"-1\"\n",
        "    \n",
        "    # Loop over counts\n",
        "    for i in range(count-1):\n",
        "        \n",
        "        # get n and nplus1 counts\n",
        "        n_gram_counts = n_gram_counts_list[i]\n",
        "        nplus1_gram_counts = n_gram_counts_list[i+1]\n",
        "        \n",
        "        # get suggestions \n",
        "        suggestion = auto_complete(previous_tokens, n_gram_counts,\n",
        "                                    nplus1_gram_counts, vocabulary,\n",
        "                                    k=k, start_with=start_with, num_suggestions=num_suggestions)\n",
        "        # Append to list\n",
        "        suggestions.append(suggestion)\n",
        "        \n",
        "    return suggestions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo7HQA4BEY_J"
      },
      "outputs": [],
      "source": [
        "n = 3\n",
        "n_gram_counts_list = []\n",
        "for n in range(n, n+2):\n",
        "    n_model_counts = count_n_grams(final_train, n)\n",
        "    n_gram_counts_list.append(n_model_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "G_z2PDHHEbE1",
        "outputId": "f6121a6a-d35e-43d4-a0bf-454e5ba7b007"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('s', 0.013278085202149912),\n",
              "  ('next', 0.009055045142842014),\n",
              "  ('a', 0.014257724588948608),\n",
              "  ('x', 0.0058778363207921845),\n",
              "  ('raw', 0.005666022399322196),\n",
              "  ('(', 0.005388016627392835),\n",
              "  ('math', 0.003044825121131086),\n",
              "  ('input', 0.5852286266514867),\n",
              "  (')', 0.0025417670576398633),\n",
              "  ('c', 0.0024623368370886175)]]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "previous_tokens = [\"n\", \"=\", \"int\", \"(\"]\n",
        "suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=2.0, num_suggestions=10)\n",
        "\n",
        "display(suggestion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiGSLn2lFN8G"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "N-Gram Programming Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}