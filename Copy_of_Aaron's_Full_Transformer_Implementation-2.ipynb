{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TACsfYha-MS3",
        "outputId": "ddde13db-2caf-4925-8034-20f9b86d3a5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "from keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Add, Embedding, Dropout, Flatten, LocallyConnected1D\n",
        "from keras.models import Model, load_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHUcXhlEpOz0"
      },
      "outputs": [],
      "source": [
        "with open('drive/MyDrive/samples/tokenizer.json') as f:\n",
        "  json_string = json.load(f)\n",
        "\n",
        "# with open('drive/MyDrive/samples/tokenizer.json', 'rb') as handle:\n",
        "#     tokenizer = pickle.load(handle)\n",
        "\n",
        "tokenizer = keras.preprocessing.text.tokenizer_from_json(\n",
        "    json_string\n",
        ")\n",
        "# keys = tokenizer.word_index.keys()\n",
        "\n",
        "# drop = []\n",
        "# drop_index = []\n",
        "\n",
        "# for key in keys:\n",
        "#   if tokenizer.word_index[key] >= 10000:\n",
        "#     drop.append(key)\n",
        "#     drop_index.append(tokenizer.word_index[key])\n",
        "\n",
        "# print(len(drop))\n",
        "\n",
        "# [tokenizer.word_index.pop(key) for key in drop]\n",
        "# [tokenizer.index_word.pop(key) for key in drop_index]\n",
        "# print(type(tokenizer.word_index))\n",
        "\n",
        "\n",
        "\n",
        "dictionary_dim = len(tokenizer.word_index)\n",
        "\n",
        "# print(dictionary_dim)\n",
        "\n",
        "# tokenizer.word_index['<PAD>'] = 0\n",
        "# tokenizer.index_word[0] = '<PAD>'\n",
        "\n",
        "# tokenizer.word_index['<START>'] = dictionary_dim-1\n",
        "# tokenizer.index_word[dictionary_dim-1] = '<START>'\n",
        "\n",
        "# tokenizer.word_index['<END>'] = dictionary_dim\n",
        "# tokenizer.index_word[dictionary_dim] = '<END>'\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9FPI_uJ1wC1",
        "outputId": "5bf0aa7f-86d2-436e-e87b-5f6550ac2a88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.word_index[\"\\n\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuqQnmmYdq-D"
      },
      "outputs": [],
      "source": [
        "# print(set(tokenizer.word_index.keys()) - set(tokenizer.index_word.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmXk4MFsLLXf"
      },
      "outputs": [],
      "source": [
        "# print(dictionary_dim)\n",
        "# print(len(tokenizer.index_word))\n",
        "# print(len(tokenizer.word_index))\n",
        "\n",
        "# diff = list(set(tokenizer.word_index.keys()) - set(tokenizer.index_word.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o5RkuWsde6_"
      },
      "outputs": [],
      "source": [
        "# for word in diff:\n",
        "#   tokenizer.word_index.pop(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlbJqVzceZzf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m2fBqet-16i",
        "outputId": "029e94c7-e880-4081-f223-c2e07f0715e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "811174\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 8)]          0           []                               \n",
            "                                                                                                  \n",
            " embedding_6 (Embedding)        (None, 8, 128)       103830272   ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 8, 128)      0           ['embedding_6[0][0]']            \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 8, 128)      527488      ['tf.__operators__.add_6[0][0]', \n",
            " eadAttention)                                                    'tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 8, 128)       0           ['multi_head_attention_2[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TFOpLa  (None, 8, 128)      0           ['tf.__operators__.add_6[0][0]', \n",
            " mbda)                                                            'dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 8, 128)      256         ['tf.__operators__.add_7[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 8, 4096)      528384      ['layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 8, 128)       524416      ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 8, 128)       0           ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TFOpLa  (None, 8, 128)      0           ['layer_normalization_4[0][0]',  \n",
            " mbda)                                                            'dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 8, 128)      256         ['tf.__operators__.add_8[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 1024)         0           ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 2048)         2099200     ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 10001)        20492049    ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 128,002,321\n",
            "Trainable params: 128,002,321\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "embedding_dim = 128\n",
        "max_input_len = 8\n",
        "num_heads = 8\n",
        "ffn_units = 4096\n",
        "dropout_rate = 0\n",
        "\n",
        "print(dictionary_dim)\n",
        "\n",
        "# https://keras.io/examples/nlp/neural_machine_translation_with_transformer/\n",
        "# def get_causal_attention_mask(inputs):\n",
        "#     input_shape = tf.shape(inputs)\n",
        "#     batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "#     i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "#     j = tf.range(sequence_length)\n",
        "#     mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "#     mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "#     mult = tf.concat(\n",
        "#         [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "#         axis=0,\n",
        "#     )\n",
        "\n",
        "#     if mask is not None:\n",
        "#       padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "#       padding_mask = tf.minimum(padding_mask, tf.tile(mask, mult))\n",
        "\n",
        "#     return [padding_mask, tf.tile(mask, mult)]\n",
        "#\n",
        "\n",
        "inp = Input(shape=(max_input_len,))\n",
        "# target = Input(shape=(max_input_len,))\n",
        "\n",
        "word_emb_1 = Embedding(input_dim=dictionary_dim, output_dim=embedding_dim)\n",
        "word_emb_2 = Embedding(input_dim=dictionary_dim, output_dim=embedding_dim)\n",
        "\n",
        "inp_emb = word_emb_1(inp)\n",
        "# target_emb = word_emb_2(target)\n",
        "\n",
        "### Postional encodings with help from https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/\n",
        "pos_emb = Embedding(input_dim=max_input_len, output_dim=embedding_dim)(tf.range(max_input_len))\n",
        "###\n",
        "\n",
        "inp_emb_sum = inp_emb + pos_emb\n",
        "# target_emb_sum = target_emb + pos_emb\n",
        "\n",
        "### START ENCODER\n",
        "\n",
        "att = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(inp_emb_sum, inp_emb_sum)\n",
        "\n",
        "dropout_1 = Dropout(dropout_rate)(att)\n",
        "\n",
        "norm_1 = LayerNormalization()(inp_emb_sum + dropout_1)\n",
        "\n",
        "dense_1 = Dense(units=ffn_units, activation=\"relu\")(norm_1)\n",
        "\n",
        "dense_2 = Dense(units=embedding_dim, activation=\"relu\")(dense_1)\n",
        "\n",
        "dropout_2 = Dropout(dropout_rate)(dense_2)\n",
        "\n",
        "norm_2 = LayerNormalization()(norm_1 + dropout_2)\n",
        "\n",
        "### END ENCODER\n",
        "\n",
        "### START DECODER\n",
        "\n",
        "# https://keras.io/examples/nlp/neural_machine_translation_with_transformer/\n",
        "\n",
        "# padding_mask, causal_mask = get_causal_attention_mask(target_emb_sum)\n",
        "\n",
        "# print(get_causal_attention_mask(target_emb_sum)[0][0].shape)\n",
        "# print(get_causal_attention_mask(target_emb_sum)[1].shape)\n",
        "\n",
        "# #\n",
        "\n",
        "# print(causal_mask, padding_mask)\n",
        "\n",
        "# # I don't understand the masking that's happening in att_2 and att_3 layers\n",
        "\n",
        "# att_2 = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(query=target_emb_sum, \n",
        "#                                                                        value=target_emb_sum, key=target_emb_sum, attention_mask=get_causal_attention_mask(target_emb_sum)[1])\n",
        "\n",
        "# norm_3 = LayerNormalization()(target_emb_sum + att_2)\n",
        "\n",
        "# att_3 = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(query=norm_3, \n",
        "#                                                                        value=norm_2, key=norm_2, attention_mask=get_causal_attention_mask(target_emb_sum)[0][0])\n",
        "\n",
        "\n",
        "flat_1 = Flatten()(norm_2)\n",
        "\n",
        "funnel = Dense(2048, activation=\"relu\")(flat_1)\n",
        "\n",
        "out = Dense(10000 + 1, activation=\"softmax\")(funnel)\n",
        "\n",
        "### END DECODER\n",
        "\n",
        "# local = LocallyConnected1D(128, 3)(norm_2)\n",
        "# local_1 = LocallyConnected1D(128, 3)(local)\n",
        "\n",
        "# flat = Flatten()(norm_2)\n",
        "\n",
        "# out = Dense(units=dictionary_dim+1, activation=\"softmax\")(flat)\n",
        "\n",
        "model = Model(inp, out)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dFPqpt1CtJU"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', \n",
        "              metrics=[\"sparse_categorical_accuracy\", \"sparse_top_k_categorical_accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-HKd0stJygp"
      },
      "outputs": [],
      "source": [
        "# tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "#     num_words=None,\n",
        "#     filters=\"@\",\n",
        "#     split=None,\n",
        "#     char_level=True,\n",
        "#     lower=False\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "evAfT0xBJ3pd"
      },
      "outputs": [],
      "source": [
        "pkl = pd.read_pickle(\"drive/MyDrive/samples/train.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H9GDXAla1zCd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a4bxf7-O1Qvc",
        "outputId": "1ac6e42a-9070-404c-bf4c-943d09545965"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ce0e5182-4bab-46e8-a6f6-7445105d4ad4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>samples</th>\n",
              "      <th>encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>325956</th>\n",
              "      <td>Project_CodeNet/data/p02629/Python/s932742175....</td>\n",
              "      <td>[n, =, int, (, input, (, ), ), \\n, l, =, list,...</td>\n",
              "      <td>[23, 6, 17, 4, 16, 4, 3, 3, 2, 53, 6, 44, 4, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>846024</th>\n",
              "      <td>Project_CodeNet/data/p02416/Python/s662498726....</td>\n",
              "      <td>[while, True, :, \\n,     , x, =, input, (, ), ...</td>\n",
              "      <td>[77, 90, 7, 2, 1, 27, 6, 16, 4, 3, 2, 20, 27, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266018</th>\n",
              "      <td>Project_CodeNet/data/p03150/Python/s061551833....</td>\n",
              "      <td>[s, =, input, (, ), \\n, import, re, \\n, if, \"k...</td>\n",
              "      <td>[34, 6, 16, 4, 3, 2, 41, 298, 2, 20, 3254, 25,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>828423</th>\n",
              "      <td>Project_CodeNet/data/p02659/Python/s855426226....</td>\n",
              "      <td>[A, ,, B, =, input, (, ), ., split, (, ' ', ),...</td>\n",
              "      <td>[38, 5, 55, 6, 16, 4, 3, 11, 26, 4, 126, 3, 2,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524841</th>\n",
              "      <td>Project_CodeNet/data/p02400/Python/s711691697....</td>\n",
              "      <td>[from, math, import, pi, \\n, r, =, float, (, i...</td>\n",
              "      <td>[66, 84, 41, 260, 2, 58, 6, 139, 4, 16, 4, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72429</th>\n",
              "      <td>Project_CodeNet/data/p03463/Python/s886095411....</td>\n",
              "      <td>[a, ,, b, ,, c, =, map, (, int, ,, input, (, )...</td>\n",
              "      <td>[22, 5, 32, 5, 42, 6, 33, 4, 17, 5, 16, 4, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1322644</th>\n",
              "      <td>Project_CodeNet/data/p03814/Python/s433791602....</td>\n",
              "      <td>[# 入力, \\n, s, =, input, (, ), \\n, \\n, # 処理, \\n...</td>\n",
              "      <td>[45, 2, 34, 6, 16, 4, 3, 2, 2, 45, 2, 1, 6, 14...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217675</th>\n",
              "      <td>Project_CodeNet/data/p04044/Python/s218572881....</td>\n",
              "      <td>[n, ,, l, =, list, (, map, (, int, ,, input, (...</td>\n",
              "      <td>[23, 5, 53, 6, 44, 4, 33, 4, 17, 5, 16, 4, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22402</th>\n",
              "      <td>Project_CodeNet/data/p03337/Python/s595507867....</td>\n",
              "      <td>[A, ,, B, =, list, (, map, (, int, ,, input, (...</td>\n",
              "      <td>[38, 5, 55, 6, 44, 4, 33, 4, 17, 5, 16, 4, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1239652</th>\n",
              "      <td>Project_CodeNet/data/p02697/Python/s124155220....</td>\n",
              "      <td>[#ABC165-E Rotation Matching, \\n, \"\"\"\\n各頂点を円形に...</td>\n",
              "      <td>[1, 2, 1, 2, 41, 51, 2, 81, 6, 51, 11, 78, 11,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1352302</th>\n",
              "      <td>Project_CodeNet/data/p02723/Python/s198909101....</td>\n",
              "      <td>[N, =, input, (, ), \\n, if, N, [, -, 4, ], ==,...</td>\n",
              "      <td>[31, 6, 16, 4, 3, 2, 20, 31, 9, 14, 115, 8, 25...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437893</th>\n",
              "      <td>Project_CodeNet/data/p03963/Python/s276884571....</td>\n",
              "      <td>[N, ,, K, =, map, (, int, ,, input, (, ), ., s...</td>\n",
              "      <td>[31, 5, 68, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1420871</th>\n",
              "      <td>Project_CodeNet/data/p03294/Python/s190561656....</td>\n",
              "      <td>[import, fractions, \\n, from, functools, impor...</td>\n",
              "      <td>[41, 214, 2, 66, 259, 41, 272, 2, 39, 1824, 4,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191104</th>\n",
              "      <td>Project_CodeNet/data/p04031/Python/s356150487....</td>\n",
              "      <td>[mini, =, 10, **, 29, \\n, n, =, int, (, input,...</td>\n",
              "      <td>[671, 6, 60, 59, 1157, 2, 23, 6, 17, 4, 16, 4,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1490229</th>\n",
              "      <td>Project_CodeNet/data/p03074/Python/s994843386....</td>\n",
              "      <td>[\\n, #input, \\n, #N = list(map(int,input().spl...</td>\n",
              "      <td>[2, 1438, 2, 2050, 2, 3268, 2, 9, 31, 5, 68, 8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244245</th>\n",
              "      <td>Project_CodeNet/data/p02676/Python/s625766879....</td>\n",
              "      <td>[k, =, int, (, input, (, ), ), \\n, s, =, input...</td>\n",
              "      <td>[43, 6, 17, 4, 16, 4, 3, 3, 2, 34, 6, 16, 4, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62678</th>\n",
              "      <td>Project_CodeNet/data/p03071/Python/s696508642....</td>\n",
              "      <td>[a, ,, b, =, map, (, int, ,, input, (, ), ., s...</td>\n",
              "      <td>[22, 5, 32, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1355165</th>\n",
              "      <td>Project_CodeNet/data/p03030/Python/s626411542....</td>\n",
              "      <td>[#B - Guidebook (別解), \\n, N, =, int, (, input,...</td>\n",
              "      <td>[1589, 2, 31, 6, 17, 4, 16, 4, 3, 3, 2, 2, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1326097</th>\n",
              "      <td>Project_CodeNet/data/p03777/Python/s131810997....</td>\n",
              "      <td>[# cook your dish here, \\n, l, =, input, (, ),...</td>\n",
              "      <td>[45, 2, 53, 6, 16, 4, 3, 11, 26, 4, 3, 2, 2, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1340505</th>\n",
              "      <td>Project_CodeNet/data/p02937/Python/s128101478....</td>\n",
              "      <td>[# -*- coding: utf-8 -*-, \\n, import, bisect, ...</td>\n",
              "      <td>[45, 2, 41, 147, 2, 2, 39, 1, 4, 3960, 5, 172,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472821</th>\n",
              "      <td>Project_CodeNet/data/p03160/Python/s702589319....</td>\n",
              "      <td>[\\n, def, frog, (, H, ,, N, ), :, \\n, #print(H...</td>\n",
              "      <td>[2, 39, 1, 4, 94, 5, 31, 3, 7, 2, 9248, 2, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>705586</th>\n",
              "      <td>Project_CodeNet/data/p02935/Python/s529875701....</td>\n",
              "      <td>[N, =, int, (, input, (, ), ), \\n, U, =, list,...</td>\n",
              "      <td>[31, 6, 17, 4, 16, 4, 3, 3, 2, 415, 6, 44, 4, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>531805</th>\n",
              "      <td>Project_CodeNet/data/p00046/Python/s095442933....</td>\n",
              "      <td>[from, decimal, import, Decimal, ,, ROUND_HALF...</td>\n",
              "      <td>[66, 445, 41, 431, 5, 2334, 2, 23, 6, 9, 8, 2,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1088926</th>\n",
              "      <td>Project_CodeNet/data/p02699/Python/s459376494....</td>\n",
              "      <td>[s, ,, t, =, map, (, int, ,, input, (, ), ., s...</td>\n",
              "      <td>[34, 5, 62, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1184218</th>\n",
              "      <td>Project_CodeNet/data/p02688/Python/s472022788....</td>\n",
              "      <td>[n, ,, k, =, map, (, int, ,, input, (, ), ., s...</td>\n",
              "      <td>[23, 5, 43, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1346183</th>\n",
              "      <td>Project_CodeNet/data/p03962/Python/s395952262....</td>\n",
              "      <td>[a, ,, b, ,, c, =, map, (, int, ,, input, (, )...</td>\n",
              "      <td>[22, 5, 32, 5, 42, 6, 33, 4, 17, 5, 16, 4, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398590</th>\n",
              "      <td>Project_CodeNet/data/p02743/Python/s347194680....</td>\n",
              "      <td>[# coding: utf-8, \\n, # Your code here!, \\n, \\...</td>\n",
              "      <td>[45, 2, 45, 2, 2, 45, 2, 45, 2, 45, 2, 41, 51,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1154178</th>\n",
              "      <td>Project_CodeNet/data/p03244/Python/s361046757....</td>\n",
              "      <td>[import, math, \\n, import, sys, \\n, \\n, n, =, ...</td>\n",
              "      <td>[41, 84, 2, 41, 51, 2, 2, 23, 6, 17, 4, 51, 11...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118143</th>\n",
              "      <td>Project_CodeNet/data/p03254/Python/s415124673....</td>\n",
              "      <td>[N, ,, M, =, map, (, int, ,, input, (, ), ., s...</td>\n",
              "      <td>[31, 5, 83, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504710</th>\n",
              "      <td>Project_CodeNet/data/p02381/Python/s142172074....</td>\n",
              "      <td>[from, math, import, sqrt, \\n, \\n, while, True...</td>\n",
              "      <td>[66, 84, 41, 220, 2, 2, 77, 90, 7, 2, 1, 23, 6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1421601</th>\n",
              "      <td>Project_CodeNet/data/p03294/Python/s886488697....</td>\n",
              "      <td>[# ABC103 C-Modulo Summation, \\n, \\n, N, =, in...</td>\n",
              "      <td>[45, 2, 2, 31, 6, 17, 4, 16, 4, 3, 3, 2, 22, 6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>674914</th>\n",
              "      <td>Project_CodeNet/data/p02771/Python/s507744756....</td>\n",
              "      <td>[abc, =, list, (, map, (, int, ,, input, (, ),...</td>\n",
              "      <td>[421, 6, 44, 4, 33, 4, 17, 5, 16, 4, 3, 11, 26...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1021955</th>\n",
              "      <td>Project_CodeNet/data/p03038/Python/s917494383....</td>\n",
              "      <td>[import, bisect, \\n, import, heapq, \\n, n, ,, ...</td>\n",
              "      <td>[41, 147, 2, 41, 149, 2, 23, 5, 56, 6, 33, 4, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1295393</th>\n",
              "      <td>Project_CodeNet/data/p02885/Python/s797694086....</td>\n",
              "      <td>[a, ,, b, =, map, (, int, ,, input, (, ), ., s...</td>\n",
              "      <td>[22, 5, 32, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>829121</th>\n",
              "      <td>Project_CodeNet/data/p02659/Python/s742194324....</td>\n",
              "      <td>[from, decimal, import, Decimal, ,, ROUND_DOWN...</td>\n",
              "      <td>[66, 445, 41, 431, 5, 9372, 2, 22, 5, 32, 6, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984217</th>\n",
              "      <td>Project_CodeNet/data/p02947/Python/s254530858....</td>\n",
              "      <td>[from, sys, import, stdin, \\n, \\n, N, =, int, ...</td>\n",
              "      <td>[66, 51, 41, 78, 2, 2, 31, 6, 17, 4, 78, 11, 8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>877464</th>\n",
              "      <td>Project_CodeNet/data/p03287/Python/s424518884....</td>\n",
              "      <td>[from, collections, import, Counter, \\n, n, ,,...</td>\n",
              "      <td>[66, 111, 41, 153, 2, 23, 5, 56, 6, 33, 4, 17,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1425464</th>\n",
              "      <td>Project_CodeNet/data/p03024/Python/s768296035....</td>\n",
              "      <td>[s, =, input, (, ), \\n, c, =, 0, \\n, n, =, lis...</td>\n",
              "      <td>[34, 6, 16, 4, 3, 2, 42, 6, 13, 2, 23, 6, 44, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597416</th>\n",
              "      <td>Project_CodeNet/data/p02753/Python/s756644018....</td>\n",
              "      <td>[s, =, input, (, ), \\n, if, s, ., find, (, 'AB...</td>\n",
              "      <td>[34, 6, 16, 4, 3, 2, 20, 34, 11, 178, 4, 2572,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>855985</th>\n",
              "      <td>Project_CodeNet/data/p02398/Python/s204807800....</td>\n",
              "      <td>[a, ,, b, ,, c, =, (, int, (, i, ), for, i, in...</td>\n",
              "      <td>[22, 5, 32, 5, 42, 6, 4, 17, 4, 12, 3, 18, 12,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1302814</th>\n",
              "      <td>Project_CodeNet/data/p03361/Python/s974807500....</td>\n",
              "      <td>[import, sys, \\n, h, ,, w, =, map, (, int, ,, ...</td>\n",
              "      <td>[41, 51, 2, 72, 5, 85, 6, 33, 4, 17, 5, 16, 4,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81200</th>\n",
              "      <td>Project_CodeNet/data/p02801/Python/s978266585....</td>\n",
              "      <td>[\"\"\"\\nauthor : halo2halo\\ndate : 4, Feb, 2020\\...</td>\n",
              "      <td>[6367, 2, 2, 41, 51, 2, 2, 45, 2, 45, 2, 2, 45...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275307</th>\n",
              "      <td>Project_CodeNet/data/p03494/Python/s143801399....</td>\n",
              "      <td>[N, =, int, (, input, (, ), ), \\n, A, =, list,...</td>\n",
              "      <td>[31, 6, 17, 4, 16, 4, 3, 3, 2, 38, 6, 44, 4, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>748908</th>\n",
              "      <td>Project_CodeNet/data/p03214/Python/s810480073....</td>\n",
              "      <td>[n, =, int, (, input, (, ), ), \\n, a, =, list,...</td>\n",
              "      <td>[23, 6, 17, 4, 16, 4, 3, 3, 2, 22, 6, 44, 4, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1242555</th>\n",
              "      <td>Project_CodeNet/data/p03544/Python/s103931093....</td>\n",
              "      <td>[N, =, int, (, input, (, ), ), \\n, k, =, {, },...</td>\n",
              "      <td>[31, 6, 17, 4, 16, 4, 3, 3, 2, 43, 6, 167, 170...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1141345</th>\n",
              "      <td>Project_CodeNet/data/p02657/Python/s747200841....</td>\n",
              "      <td>[a, ,, b, =, map, (, int, ,, input, (, ), ., s...</td>\n",
              "      <td>[22, 5, 32, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153730</th>\n",
              "      <td>Project_CodeNet/data/p02785/Python/s842193630....</td>\n",
              "      <td>[X, =, list, (, map, (, int, ,, input, (, ), ....</td>\n",
              "      <td>[80, 6, 44, 4, 33, 4, 17, 5, 16, 4, 3, 11, 26,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149148</th>\n",
              "      <td>Project_CodeNet/data/p02766/Python/s087562467....</td>\n",
              "      <td>[N, ,, K, =, map, (, int, ,, input, (, ), ., s...</td>\n",
              "      <td>[31, 5, 68, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>666112</th>\n",
              "      <td>Project_CodeNet/data/p03994/Python/s717505940....</td>\n",
              "      <td>[# -*- coding: utf-8 -*-, \\n, # !/usr/bin/env ...</td>\n",
              "      <td>[45, 2, 45, 2, 45, 2, 2, 1, 2, 41, 51, 2, 2, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438308</th>\n",
              "      <td>Project_CodeNet/data/p03963/Python/s793803722....</td>\n",
              "      <td>[n, ,, k, =, map, (, int, ,, input, (, ), ., s...</td>\n",
              "      <td>[23, 5, 43, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce0e5182-4bab-46e8-a6f6-7445105d4ad4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ce0e5182-4bab-46e8-a6f6-7445105d4ad4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ce0e5182-4bab-46e8-a6f6-7445105d4ad4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                      path  \\\n",
              "325956   Project_CodeNet/data/p02629/Python/s932742175....   \n",
              "846024   Project_CodeNet/data/p02416/Python/s662498726....   \n",
              "266018   Project_CodeNet/data/p03150/Python/s061551833....   \n",
              "828423   Project_CodeNet/data/p02659/Python/s855426226....   \n",
              "524841   Project_CodeNet/data/p02400/Python/s711691697....   \n",
              "72429    Project_CodeNet/data/p03463/Python/s886095411....   \n",
              "1322644  Project_CodeNet/data/p03814/Python/s433791602....   \n",
              "217675   Project_CodeNet/data/p04044/Python/s218572881....   \n",
              "22402    Project_CodeNet/data/p03337/Python/s595507867....   \n",
              "1239652  Project_CodeNet/data/p02697/Python/s124155220....   \n",
              "1352302  Project_CodeNet/data/p02723/Python/s198909101....   \n",
              "437893   Project_CodeNet/data/p03963/Python/s276884571....   \n",
              "1420871  Project_CodeNet/data/p03294/Python/s190561656....   \n",
              "191104   Project_CodeNet/data/p04031/Python/s356150487....   \n",
              "1490229  Project_CodeNet/data/p03074/Python/s994843386....   \n",
              "244245   Project_CodeNet/data/p02676/Python/s625766879....   \n",
              "62678    Project_CodeNet/data/p03071/Python/s696508642....   \n",
              "1355165  Project_CodeNet/data/p03030/Python/s626411542....   \n",
              "1326097  Project_CodeNet/data/p03777/Python/s131810997....   \n",
              "1340505  Project_CodeNet/data/p02937/Python/s128101478....   \n",
              "472821   Project_CodeNet/data/p03160/Python/s702589319....   \n",
              "705586   Project_CodeNet/data/p02935/Python/s529875701....   \n",
              "531805   Project_CodeNet/data/p00046/Python/s095442933....   \n",
              "1088926  Project_CodeNet/data/p02699/Python/s459376494....   \n",
              "1184218  Project_CodeNet/data/p02688/Python/s472022788....   \n",
              "1346183  Project_CodeNet/data/p03962/Python/s395952262....   \n",
              "398590   Project_CodeNet/data/p02743/Python/s347194680....   \n",
              "1154178  Project_CodeNet/data/p03244/Python/s361046757....   \n",
              "118143   Project_CodeNet/data/p03254/Python/s415124673....   \n",
              "504710   Project_CodeNet/data/p02381/Python/s142172074....   \n",
              "1421601  Project_CodeNet/data/p03294/Python/s886488697....   \n",
              "674914   Project_CodeNet/data/p02771/Python/s507744756....   \n",
              "1021955  Project_CodeNet/data/p03038/Python/s917494383....   \n",
              "1295393  Project_CodeNet/data/p02885/Python/s797694086....   \n",
              "829121   Project_CodeNet/data/p02659/Python/s742194324....   \n",
              "984217   Project_CodeNet/data/p02947/Python/s254530858....   \n",
              "877464   Project_CodeNet/data/p03287/Python/s424518884....   \n",
              "1425464  Project_CodeNet/data/p03024/Python/s768296035....   \n",
              "597416   Project_CodeNet/data/p02753/Python/s756644018....   \n",
              "855985   Project_CodeNet/data/p02398/Python/s204807800....   \n",
              "1302814  Project_CodeNet/data/p03361/Python/s974807500....   \n",
              "81200    Project_CodeNet/data/p02801/Python/s978266585....   \n",
              "275307   Project_CodeNet/data/p03494/Python/s143801399....   \n",
              "748908   Project_CodeNet/data/p03214/Python/s810480073....   \n",
              "1242555  Project_CodeNet/data/p03544/Python/s103931093....   \n",
              "1141345  Project_CodeNet/data/p02657/Python/s747200841....   \n",
              "153730   Project_CodeNet/data/p02785/Python/s842193630....   \n",
              "149148   Project_CodeNet/data/p02766/Python/s087562467....   \n",
              "666112   Project_CodeNet/data/p03994/Python/s717505940....   \n",
              "438308   Project_CodeNet/data/p03963/Python/s793803722....   \n",
              "\n",
              "                                                   samples  \\\n",
              "325956   [n, =, int, (, input, (, ), ), \\n, l, =, list,...   \n",
              "846024   [while, True, :, \\n,     , x, =, input, (, ), ...   \n",
              "266018   [s, =, input, (, ), \\n, import, re, \\n, if, \"k...   \n",
              "828423   [A, ,, B, =, input, (, ), ., split, (, ' ', ),...   \n",
              "524841   [from, math, import, pi, \\n, r, =, float, (, i...   \n",
              "72429    [a, ,, b, ,, c, =, map, (, int, ,, input, (, )...   \n",
              "1322644  [# 入力, \\n, s, =, input, (, ), \\n, \\n, # 処理, \\n...   \n",
              "217675   [n, ,, l, =, list, (, map, (, int, ,, input, (...   \n",
              "22402    [A, ,, B, =, list, (, map, (, int, ,, input, (...   \n",
              "1239652  [#ABC165-E Rotation Matching, \\n, \"\"\"\\n各頂点を円形に...   \n",
              "1352302  [N, =, input, (, ), \\n, if, N, [, -, 4, ], ==,...   \n",
              "437893   [N, ,, K, =, map, (, int, ,, input, (, ), ., s...   \n",
              "1420871  [import, fractions, \\n, from, functools, impor...   \n",
              "191104   [mini, =, 10, **, 29, \\n, n, =, int, (, input,...   \n",
              "1490229  [\\n, #input, \\n, #N = list(map(int,input().spl...   \n",
              "244245   [k, =, int, (, input, (, ), ), \\n, s, =, input...   \n",
              "62678    [a, ,, b, =, map, (, int, ,, input, (, ), ., s...   \n",
              "1355165  [#B - Guidebook (別解), \\n, N, =, int, (, input,...   \n",
              "1326097  [# cook your dish here, \\n, l, =, input, (, ),...   \n",
              "1340505  [# -*- coding: utf-8 -*-, \\n, import, bisect, ...   \n",
              "472821   [\\n, def, frog, (, H, ,, N, ), :, \\n, #print(H...   \n",
              "705586   [N, =, int, (, input, (, ), ), \\n, U, =, list,...   \n",
              "531805   [from, decimal, import, Decimal, ,, ROUND_HALF...   \n",
              "1088926  [s, ,, t, =, map, (, int, ,, input, (, ), ., s...   \n",
              "1184218  [n, ,, k, =, map, (, int, ,, input, (, ), ., s...   \n",
              "1346183  [a, ,, b, ,, c, =, map, (, int, ,, input, (, )...   \n",
              "398590   [# coding: utf-8, \\n, # Your code here!, \\n, \\...   \n",
              "1154178  [import, math, \\n, import, sys, \\n, \\n, n, =, ...   \n",
              "118143   [N, ,, M, =, map, (, int, ,, input, (, ), ., s...   \n",
              "504710   [from, math, import, sqrt, \\n, \\n, while, True...   \n",
              "1421601  [# ABC103 C-Modulo Summation, \\n, \\n, N, =, in...   \n",
              "674914   [abc, =, list, (, map, (, int, ,, input, (, ),...   \n",
              "1021955  [import, bisect, \\n, import, heapq, \\n, n, ,, ...   \n",
              "1295393  [a, ,, b, =, map, (, int, ,, input, (, ), ., s...   \n",
              "829121   [from, decimal, import, Decimal, ,, ROUND_DOWN...   \n",
              "984217   [from, sys, import, stdin, \\n, \\n, N, =, int, ...   \n",
              "877464   [from, collections, import, Counter, \\n, n, ,,...   \n",
              "1425464  [s, =, input, (, ), \\n, c, =, 0, \\n, n, =, lis...   \n",
              "597416   [s, =, input, (, ), \\n, if, s, ., find, (, 'AB...   \n",
              "855985   [a, ,, b, ,, c, =, (, int, (, i, ), for, i, in...   \n",
              "1302814  [import, sys, \\n, h, ,, w, =, map, (, int, ,, ...   \n",
              "81200    [\"\"\"\\nauthor : halo2halo\\ndate : 4, Feb, 2020\\...   \n",
              "275307   [N, =, int, (, input, (, ), ), \\n, A, =, list,...   \n",
              "748908   [n, =, int, (, input, (, ), ), \\n, a, =, list,...   \n",
              "1242555  [N, =, int, (, input, (, ), ), \\n, k, =, {, },...   \n",
              "1141345  [a, ,, b, =, map, (, int, ,, input, (, ), ., s...   \n",
              "153730   [X, =, list, (, map, (, int, ,, input, (, ), ....   \n",
              "149148   [N, ,, K, =, map, (, int, ,, input, (, ), ., s...   \n",
              "666112   [# -*- coding: utf-8 -*-, \\n, # !/usr/bin/env ...   \n",
              "438308   [n, ,, k, =, map, (, int, ,, input, (, ), ., s...   \n",
              "\n",
              "                                                   encoded  \n",
              "325956   [23, 6, 17, 4, 16, 4, 3, 3, 2, 53, 6, 44, 4, 1...  \n",
              "846024   [77, 90, 7, 2, 1, 27, 6, 16, 4, 3, 2, 20, 27, ...  \n",
              "266018   [34, 6, 16, 4, 3, 2, 41, 298, 2, 20, 3254, 25,...  \n",
              "828423   [38, 5, 55, 6, 16, 4, 3, 11, 26, 4, 126, 3, 2,...  \n",
              "524841   [66, 84, 41, 260, 2, 58, 6, 139, 4, 16, 4, 3, ...  \n",
              "72429    [22, 5, 32, 5, 42, 6, 33, 4, 17, 5, 16, 4, 3, ...  \n",
              "1322644  [45, 2, 34, 6, 16, 4, 3, 2, 2, 45, 2, 1, 6, 14...  \n",
              "217675   [23, 5, 53, 6, 44, 4, 33, 4, 17, 5, 16, 4, 3, ...  \n",
              "22402    [38, 5, 55, 6, 44, 4, 33, 4, 17, 5, 16, 4, 3, ...  \n",
              "1239652  [1, 2, 1, 2, 41, 51, 2, 81, 6, 51, 11, 78, 11,...  \n",
              "1352302  [31, 6, 16, 4, 3, 2, 20, 31, 9, 14, 115, 8, 25...  \n",
              "437893   [31, 5, 68, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...  \n",
              "1420871  [41, 214, 2, 66, 259, 41, 272, 2, 39, 1824, 4,...  \n",
              "191104   [671, 6, 60, 59, 1157, 2, 23, 6, 17, 4, 16, 4,...  \n",
              "1490229  [2, 1438, 2, 2050, 2, 3268, 2, 9, 31, 5, 68, 8...  \n",
              "244245   [43, 6, 17, 4, 16, 4, 3, 3, 2, 34, 6, 16, 4, 3...  \n",
              "62678    [22, 5, 32, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...  \n",
              "1355165  [1589, 2, 31, 6, 17, 4, 16, 4, 3, 3, 2, 2, 1, ...  \n",
              "1326097  [45, 2, 53, 6, 16, 4, 3, 11, 26, 4, 3, 2, 2, 2...  \n",
              "1340505  [45, 2, 41, 147, 2, 2, 39, 1, 4, 3960, 5, 172,...  \n",
              "472821   [2, 39, 1, 4, 94, 5, 31, 3, 7, 2, 9248, 2, 1, ...  \n",
              "705586   [31, 6, 17, 4, 16, 4, 3, 3, 2, 415, 6, 44, 4, ...  \n",
              "531805   [66, 445, 41, 431, 5, 2334, 2, 23, 6, 9, 8, 2,...  \n",
              "1088926  [34, 5, 62, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...  \n",
              "1184218  [23, 5, 43, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...  \n",
              "1346183  [22, 5, 32, 5, 42, 6, 33, 4, 17, 5, 16, 4, 3, ...  \n",
              "398590   [45, 2, 45, 2, 2, 45, 2, 45, 2, 45, 2, 41, 51,...  \n",
              "1154178  [41, 84, 2, 41, 51, 2, 2, 23, 6, 17, 4, 51, 11...  \n",
              "118143   [31, 5, 83, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...  \n",
              "504710   [66, 84, 41, 220, 2, 2, 77, 90, 7, 2, 1, 23, 6...  \n",
              "1421601  [45, 2, 2, 31, 6, 17, 4, 16, 4, 3, 3, 2, 22, 6...  \n",
              "674914   [421, 6, 44, 4, 33, 4, 17, 5, 16, 4, 3, 11, 26...  \n",
              "1021955  [41, 147, 2, 41, 149, 2, 23, 5, 56, 6, 33, 4, ...  \n",
              "1295393  [22, 5, 32, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...  \n",
              "829121   [66, 445, 41, 431, 5, 9372, 2, 22, 5, 32, 6, 1...  \n",
              "984217   [66, 51, 41, 78, 2, 2, 31, 6, 17, 4, 78, 11, 8...  \n",
              "877464   [66, 111, 41, 153, 2, 23, 5, 56, 6, 33, 4, 17,...  \n",
              "1425464  [34, 6, 16, 4, 3, 2, 42, 6, 13, 2, 23, 6, 44, ...  \n",
              "597416   [34, 6, 16, 4, 3, 2, 20, 34, 11, 178, 4, 2572,...  \n",
              "855985   [22, 5, 32, 5, 42, 6, 4, 17, 4, 12, 3, 18, 12,...  \n",
              "1302814  [41, 51, 2, 72, 5, 85, 6, 33, 4, 17, 5, 16, 4,...  \n",
              "81200    [6367, 2, 2, 41, 51, 2, 2, 45, 2, 45, 2, 2, 45...  \n",
              "275307   [31, 6, 17, 4, 16, 4, 3, 3, 2, 38, 6, 44, 4, 3...  \n",
              "748908   [23, 6, 17, 4, 16, 4, 3, 3, 2, 22, 6, 44, 4, 3...  \n",
              "1242555  [31, 6, 17, 4, 16, 4, 3, 3, 2, 43, 6, 167, 170...  \n",
              "1141345  [22, 5, 32, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...  \n",
              "153730   [80, 6, 44, 4, 33, 4, 17, 5, 16, 4, 3, 11, 26,...  \n",
              "149148   [31, 5, 68, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...  \n",
              "666112   [45, 2, 45, 2, 45, 2, 2, 1, 2, 41, 51, 2, 2, 3...  \n",
              "438308   [23, 5, 43, 6, 33, 4, 17, 5, 16, 4, 3, 11, 26,...  "
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pkl.head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6TEdufcM10I0"
      },
      "outputs": [],
      "source": [
        "# pkl[\"encoded_2\"] = pkl[\"samples\"].apply(lambda x: [y[0] if len(y) != 0 else 1 for y in tokenizer.texts_to_sequences(x)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IGFzpJr03TwA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Bgy-TdezRZUe"
      },
      "outputs": [],
      "source": [
        "# j = 0\n",
        "\n",
        "# for idx, row in pkl.iterrows():\n",
        "#     j += 1\n",
        "\n",
        "#     if j% 10000 == 0:\n",
        "#       print(j)\n",
        "#     cur_seq = pkl.loc[idx][\"samples\"]\n",
        "#     #print(cur_seq)\n",
        "#     tokenizer.fit_on_texts(cur_seq)\n",
        "\n",
        "# j = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bvnietApXkKl"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "igJtad_8X1PB"
      },
      "outputs": [],
      "source": [
        "# with open('drive/MyDrive/samples/char_tokenizer.pkl', 'wb') as handle:\n",
        "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2ChWNI3Httd4",
        "outputId": "9ac28580-d0f7-49bd-c7d4-19324eaa09e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 8)]          0           []                               \n",
            "                                                                                                  \n",
            " embedding_6 (Embedding)        (None, 8, 128)       103830272   ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 8, 128)      0           ['embedding_6[0][0]']            \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 8, 128)      527488      ['tf.__operators__.add_6[0][0]', \n",
            " eadAttention)                                                    'tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 8, 128)       0           ['multi_head_attention_2[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TFOpLa  (None, 8, 128)      0           ['tf.__operators__.add_6[0][0]', \n",
            " mbda)                                                            'dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 8, 128)      256         ['tf.__operators__.add_7[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 8, 4096)      528384      ['layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 8, 128)       524416      ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 8, 128)       0           ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TFOpLa  (None, 8, 128)      0           ['layer_normalization_4[0][0]',  \n",
            " mbda)                                                            'dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 8, 128)      256         ['tf.__operators__.add_8[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 1024)         0           ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 2048)         2099200     ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 10001)        20492049    ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 128,002,321\n",
            "Trainable params: 128,002,321\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# model = load_model(\"drive/MyDrive/samples/transformer_model.h5\")\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', \n",
        "              metrics=[\"sparse_categorical_accuracy\", \"sparse_top_k_categorical_accuracy\"])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QPaDaV6jKgml",
        "outputId": "6fcbb4c7-3e0d-4490-e928-e71b8be2c3c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "index is:  1000\n",
            "index is:  2000\n",
            "index is:  3000\n",
            "index is:  4000\n",
            "index is:  5000\n",
            "index is:  6000\n",
            "index is:  7000\n",
            "index is:  8000\n",
            "index is:  9000\n",
            "index is:  10000\n",
            "index is:  11000\n",
            "index is:  12000\n",
            "index is:  13000\n",
            "index is:  14000\n",
            "index is:  15000\n",
            "index is:  16000\n",
            "index is:  17000\n",
            "index is:  18000\n",
            "index is:  19000\n",
            "index is:  20000\n",
            "index is:  21000\n",
            "index is:  22000\n",
            "index is:  23000\n",
            "index is:  24000\n",
            "index is:  25000\n",
            "index is:  26000\n",
            "index is:  27000\n",
            "index is:  28000\n",
            "index is:  29000\n",
            "index is:  30000\n",
            "index is:  31000\n",
            "index is:  32000\n",
            "index is:  33000\n",
            "index is:  34000\n",
            "index is:  35000\n",
            "index is:  36000\n",
            "index is:  37000\n",
            "index is:  38000\n",
            "index is:  39000\n",
            "index is:  40000\n",
            "index is:  41000\n",
            "index is:  42000\n",
            "index is:  43000\n",
            "index is:  44000\n",
            "index is:  45000\n",
            "index is:  46000\n",
            "index is:  47000\n",
            "index is:  48000\n",
            "index is:  49000\n",
            "index is:  50000\n",
            "index is:  51000\n",
            "index is:  52000\n",
            "index is:  53000\n",
            "index is:  54000\n",
            "index is:  55000\n",
            "index is:  56000\n",
            "index is:  57000\n",
            "index is:  58000\n",
            "index is:  59000\n",
            "index is:  60000\n",
            "index is:  61000\n",
            "index is:  62000\n",
            "index is:  63000\n",
            "index is:  64000\n",
            "index is:  65000\n",
            "index is:  66000\n",
            "index is:  67000\n",
            "index is:  68000\n",
            "index is:  69000\n",
            "index is:  70000\n",
            "index is:  71000\n",
            "index is:  72000\n",
            "index is:  73000\n",
            "index is:  74000\n",
            "index is:  75000\n",
            "index is:  76000\n",
            "index is:  77000\n",
            "index is:  78000\n",
            "index is:  79000\n",
            "index is:  80000\n",
            "index is:  81000\n",
            "index is:  82000\n",
            "index is:  83000\n",
            "index is:  84000\n",
            "index is:  85000\n",
            "index is:  86000\n",
            "index is:  87000\n",
            "index is:  88000\n",
            "index is:  89000\n",
            "index is:  90000\n",
            "index is:  91000\n",
            "index is:  92000\n",
            "index is:  93000\n",
            "index is:  94000\n",
            "index is:  95000\n",
            "index is:  96000\n",
            "index is:  97000\n",
            "index is:  98000\n",
            "index is:  99000\n",
            "index is:  100000\n",
            "index is:  101000\n",
            "index is:  102000\n",
            "index is:  103000\n",
            "index is:  104000\n",
            "index is:  105000\n",
            "index is:  106000\n",
            "index is:  107000\n",
            "index is:  108000\n",
            "index is:  109000\n",
            "index is:  110000\n",
            "index is:  111000\n",
            "index is:  112000\n",
            "index is:  113000\n",
            "index is:  114000\n",
            "index is:  115000\n",
            "index is:  116000\n",
            "index is:  117000\n",
            "index is:  118000\n",
            "index is:  119000\n",
            "index is:  120000\n",
            "index is:  121000\n",
            "index is:  122000\n",
            "index is:  123000\n",
            "index is:  124000\n",
            "index is:  125000\n",
            "index is:  126000\n",
            "index is:  127000\n",
            "index is:  128000\n",
            "index is:  129000\n",
            "index is:  130000\n",
            "index is:  131000\n",
            "index is:  132000\n",
            "index is:  133000\n",
            "index is:  134000\n",
            "index is:  135000\n",
            "index is:  136000\n",
            "index is:  137000\n",
            "index is:  138000\n",
            "index is:  139000\n",
            "index is:  140000\n",
            "index is:  141000\n",
            "index is:  142000\n",
            "index is:  143000\n",
            "index is:  144000\n",
            "index is:  145000\n",
            "index is:  146000\n",
            "index is:  147000\n",
            "index is:  148000\n",
            "index is:  149000\n",
            "index is:  150000\n",
            "index is:  151000\n",
            "index is:  152000\n",
            "index is:  153000\n",
            "index is:  154000\n",
            "index is:  155000\n",
            "index is:  156000\n",
            "index is:  157000\n",
            "index is:  158000\n",
            "index is:  159000\n",
            "index is:  160000\n",
            "index is:  161000\n",
            "index is:  162000\n",
            "index is:  163000\n",
            "index is:  164000\n",
            "index is:  165000\n",
            "index is:  166000\n",
            "index is:  167000\n",
            "index is:  168000\n",
            "index is:  169000\n",
            "index is:  170000\n",
            "index is:  171000\n",
            "index is:  172000\n",
            "index is:  173000\n",
            "index is:  174000\n",
            "index is:  175000\n",
            "index is:  176000\n",
            "index is:  177000\n",
            "index is:  178000\n",
            "index is:  179000\n",
            "index is:  180000\n",
            "index is:  181000\n",
            "index is:  182000\n",
            "index is:  183000\n",
            "index is:  184000\n",
            "index is:  185000\n",
            "index is:  186000\n",
            "index is:  187000\n",
            "index is:  188000\n",
            "index is:  189000\n",
            "index is:  190000\n",
            "index is:  191000\n",
            "index is:  192000\n",
            "index is:  193000\n",
            "index is:  194000\n",
            "index is:  195000\n",
            "index is:  196000\n",
            "index is:  197000\n",
            "index is:  198000\n",
            "index is:  199000\n",
            "index is:  200000\n",
            "index is:  201000\n",
            "index is:  202000\n",
            "index is:  203000\n",
            "index is:  204000\n",
            "index is:  205000\n",
            "index is:  206000\n",
            "index is:  207000\n",
            "index is:  208000\n",
            "index is:  209000\n",
            "index is:  210000\n",
            "index is:  211000\n",
            "index is:  212000\n",
            "index is:  213000\n",
            "index is:  214000\n",
            "index is:  215000\n",
            "index is:  216000\n",
            "index is:  217000\n",
            "index is:  218000\n",
            "index is:  219000\n",
            "index is:  220000\n",
            "index is:  221000\n",
            "index is:  222000\n",
            "index is:  223000\n",
            "index is:  224000\n",
            "index is:  225000\n",
            "index is:  226000\n",
            "index is:  227000\n",
            "index is:  228000\n",
            "index is:  229000\n",
            "index is:  230000\n",
            "index is:  231000\n",
            "index is:  232000\n",
            "index is:  233000\n",
            "index is:  234000\n",
            "index is:  235000\n",
            "index is:  236000\n",
            "index is:  237000\n",
            "index is:  238000\n",
            "index is:  239000\n",
            "index is:  240000\n",
            "index is:  241000\n",
            "index is:  242000\n",
            "index is:  243000\n",
            "index is:  244000\n",
            "index is:  245000\n",
            "index is:  246000\n",
            "index is:  247000\n",
            "index is:  248000\n",
            "index is:  249000\n",
            "index is:  250000\n",
            "index is:  251000\n",
            "index is:  252000\n",
            "index is:  253000\n",
            "index is:  254000\n",
            "index is:  255000\n",
            "index is:  256000\n",
            "index is:  257000\n",
            "index is:  258000\n",
            "index is:  259000\n",
            "index is:  260000\n",
            "index is:  261000\n",
            "index is:  262000\n",
            "index is:  263000\n",
            "index is:  264000\n",
            "index is:  265000\n",
            "index is:  266000\n",
            "index is:  267000\n",
            "index is:  268000\n",
            "index is:  269000\n",
            "index is:  270000\n",
            "index is:  271000\n",
            "index is:  272000\n",
            "index is:  273000\n",
            "index is:  274000\n",
            "index is:  275000\n",
            "index is:  276000\n",
            "index is:  277000\n",
            "index is:  278000\n",
            "index is:  279000\n",
            "index is:  280000\n",
            "index is:  281000\n",
            "index is:  282000\n",
            "index is:  283000\n",
            "index is:  284000\n",
            "index is:  285000\n",
            "index is:  286000\n",
            "index is:  287000\n",
            "index is:  288000\n",
            "index is:  289000\n",
            "index is:  290000\n",
            "index is:  291000\n",
            "index is:  292000\n",
            "index is:  293000\n",
            "index is:  294000\n",
            "index is:  295000\n",
            "index is:  296000\n",
            "index is:  297000\n",
            "index is:  298000\n",
            "index is:  299000\n",
            "index is:  300000\n",
            "index is:  301000\n",
            "index is:  302000\n",
            "index is:  303000\n",
            "index is:  304000\n",
            "index is:  305000\n",
            "index is:  306000\n",
            "index is:  307000\n",
            "index is:  308000\n",
            "index is:  309000\n",
            "index is:  310000\n",
            "index is:  311000\n",
            "index is:  312000\n",
            "index is:  313000\n",
            "index is:  314000\n",
            "index is:  315000\n",
            "index is:  316000\n",
            "index is:  317000\n",
            "index is:  318000\n",
            "index is:  319000\n",
            "index is:  320000\n",
            "index is:  321000\n",
            "index is:  322000\n",
            "index is:  323000\n",
            "index is:  324000\n",
            "index is:  325000\n",
            "index is:  326000\n",
            "index is:  327000\n",
            "index is:  328000\n",
            "index is:  329000\n",
            "index is:  330000\n",
            "index is:  331000\n",
            "index is:  332000\n",
            "index is:  333000\n",
            "index is:  334000\n",
            "index is:  335000\n",
            "index is:  336000\n",
            "index is:  337000\n",
            "index is:  338000\n",
            "index is:  339000\n",
            "index is:  340000\n",
            "index is:  341000\n",
            "index is:  342000\n",
            "index is:  343000\n",
            "index is:  344000\n",
            "index is:  345000\n",
            "index is:  346000\n",
            "index is:  347000\n",
            "index is:  348000\n",
            "index is:  349000\n",
            "index is:  350000\n",
            "index is:  351000\n",
            "index is:  352000\n",
            "index is:  353000\n",
            "index is:  354000\n",
            "index is:  355000\n",
            "index is:  356000\n",
            "index is:  357000\n",
            "index is:  358000\n",
            "index is:  359000\n",
            "index is:  360000\n",
            "index is:  361000\n",
            "index is:  362000\n",
            "index is:  363000\n",
            "index is:  364000\n",
            "index is:  365000\n",
            "index is:  366000\n",
            "index is:  367000\n",
            "index is:  368000\n",
            "index is:  369000\n",
            "index is:  370000\n",
            "index is:  371000\n",
            "index is:  372000\n",
            "index is:  373000\n",
            "index is:  374000\n",
            "index is:  375000\n",
            "index is:  376000\n",
            "index is:  377000\n",
            "index is:  378000\n",
            "index is:  379000\n",
            "index is:  380000\n",
            "index is:  381000\n",
            "index is:  382000\n",
            "index is:  383000\n",
            "index is:  384000\n",
            "index is:  385000\n",
            "index is:  386000\n",
            "index is:  387000\n",
            "index is:  388000\n",
            "index is:  389000\n",
            "index is:  390000\n",
            "index is:  391000\n",
            "index is:  392000\n",
            "index is:  393000\n",
            "index is:  394000\n",
            "index is:  395000\n",
            "index is:  396000\n",
            "index is:  397000\n",
            "index is:  398000\n",
            "index is:  399000\n",
            "index is:  400000\n",
            "index is:  401000\n",
            "index is:  402000\n",
            "index is:  403000\n",
            "index is:  404000\n",
            "index is:  405000\n",
            "index is:  406000\n",
            "index is:  407000\n",
            "index is:  408000\n",
            "index is:  409000\n",
            "index is:  410000\n",
            "index is:  411000\n",
            "index is:  412000\n",
            "index is:  413000\n",
            "index is:  414000\n",
            "index is:  415000\n",
            "index is:  416000\n",
            "index is:  417000\n",
            "index is:  418000\n",
            "index is:  419000\n",
            "index is:  420000\n",
            "index is:  421000\n",
            "index is:  422000\n",
            "index is:  423000\n",
            "index is:  424000\n",
            "index is:  425000\n",
            "index is:  426000\n",
            "index is:  427000\n",
            "index is:  428000\n",
            "index is:  429000\n",
            "index is:  430000\n",
            "index is:  431000\n",
            "index is:  432000\n",
            "index is:  433000\n",
            "index is:  434000\n",
            "index is:  435000\n",
            "index is:  436000\n",
            "index is:  437000\n",
            "index is:  438000\n",
            "index is:  439000\n",
            "index is:  440000\n",
            "index is:  441000\n",
            "index is:  442000\n",
            "index is:  443000\n",
            "index is:  444000\n",
            "index is:  445000\n",
            "index is:  446000\n",
            "index is:  447000\n",
            "index is:  448000\n",
            "index is:  449000\n",
            "index is:  450000\n",
            "index is:  451000\n",
            "index is:  452000\n",
            "index is:  453000\n",
            "index is:  454000\n",
            "index is:  455000\n",
            "index is:  456000\n",
            "index is:  457000\n",
            "index is:  458000\n",
            "index is:  459000\n",
            "index is:  460000\n",
            "index is:  461000\n",
            "index is:  462000\n",
            "index is:  463000\n",
            "index is:  464000\n",
            "index is:  465000\n",
            "index is:  466000\n",
            "index is:  467000\n",
            "index is:  468000\n",
            "index is:  469000\n",
            "index is:  470000\n",
            "index is:  471000\n",
            "index is:  472000\n",
            "index is:  473000\n",
            "index is:  474000\n",
            "index is:  475000\n",
            "index is:  476000\n",
            "index is:  477000\n",
            "index is:  478000\n",
            "index is:  479000\n",
            "index is:  480000\n",
            "index is:  481000\n",
            "index is:  482000\n",
            "index is:  483000\n",
            "index is:  484000\n",
            "index is:  485000\n",
            "index is:  486000\n",
            "index is:  487000\n",
            "index is:  488000\n",
            "index is:  489000\n",
            "index is:  490000\n",
            "index is:  491000\n",
            "index is:  492000\n",
            "index is:  493000\n",
            "index is:  494000\n",
            "index is:  495000\n",
            "index is:  496000\n",
            "index is:  497000\n",
            "index is:  498000\n",
            "index is:  499000\n",
            "index is:  500000\n",
            "index is:  501000\n",
            "index is:  502000\n",
            "index is:  503000\n",
            "index is:  504000\n",
            "index is:  505000\n",
            "index is:  506000\n",
            "index is:  507000\n",
            "index is:  508000\n",
            "index is:  509000\n",
            "index is:  510000\n",
            "index is:  511000\n",
            "index is:  512000\n",
            "index is:  513000\n",
            "index is:  514000\n",
            "index is:  515000\n",
            "index is:  516000\n",
            "index is:  517000\n",
            "index is:  518000\n",
            "index is:  519000\n",
            "index is:  520000\n",
            "index is:  521000\n",
            "index is:  522000\n",
            "index is:  523000\n",
            "index is:  524000\n",
            "index is:  525000\n",
            "index is:  526000\n",
            "index is:  527000\n",
            "index is:  528000\n",
            "index is:  529000\n",
            "index is:  530000\n",
            "index is:  531000\n",
            "index is:  532000\n",
            "index is:  533000\n",
            "index is:  534000\n",
            "index is:  535000\n",
            "index is:  536000\n",
            "index is:  537000\n",
            "index is:  538000\n",
            "index is:  539000\n",
            "index is:  540000\n",
            "index is:  541000\n",
            "index is:  542000\n",
            "index is:  543000\n",
            "index is:  544000\n",
            "index is:  545000\n",
            "index is:  546000\n",
            "index is:  547000\n",
            "index is:  548000\n",
            "index is:  549000\n",
            "index is:  550000\n",
            "index is:  551000\n",
            "index is:  552000\n",
            "index is:  553000\n",
            "index is:  554000\n",
            "index is:  555000\n",
            "index is:  556000\n",
            "index is:  557000\n",
            "index is:  558000\n",
            "index is:  559000\n",
            "index is:  560000\n",
            "index is:  561000\n",
            "index is:  562000\n",
            "index is:  563000\n",
            "index is:  564000\n",
            "index is:  565000\n",
            "index is:  566000\n",
            "index is:  567000\n",
            "index is:  568000\n",
            "index is:  569000\n",
            "index is:  570000\n",
            "index is:  571000\n",
            "index is:  572000\n",
            "index is:  573000\n",
            "index is:  574000\n",
            "index is:  575000\n",
            "index is:  576000\n",
            "index is:  577000\n",
            "index is:  578000\n",
            "index is:  579000\n",
            "index is:  580000\n",
            "index is:  581000\n",
            "index is:  582000\n",
            "index is:  583000\n",
            "index is:  584000\n",
            "index is:  585000\n",
            "index is:  586000\n",
            "index is:  587000\n",
            "index is:  588000\n",
            "index is:  589000\n",
            "index is:  590000\n",
            "index is:  591000\n",
            "index is:  592000\n",
            "index is:  593000\n",
            "index is:  594000\n",
            "index is:  595000\n",
            "index is:  596000\n",
            "index is:  597000\n",
            "index is:  598000\n",
            "index is:  599000\n",
            "index is:  600000\n",
            "index is:  601000\n",
            "index is:  602000\n",
            "index is:  603000\n",
            "index is:  604000\n",
            "index is:  605000\n",
            "index is:  606000\n",
            "index is:  607000\n",
            "index is:  608000\n",
            "index is:  609000\n",
            "index is:  610000\n",
            "index is:  611000\n",
            "index is:  612000\n",
            "index is:  613000\n",
            "index is:  614000\n",
            "index is:  615000\n",
            "index is:  616000\n",
            "index is:  617000\n",
            "index is:  618000\n",
            "index is:  619000\n",
            "index is:  620000\n",
            "index is:  621000\n",
            "index is:  622000\n",
            "index is:  623000\n",
            "index is:  624000\n",
            "index is:  625000\n",
            "index is:  626000\n",
            "index is:  627000\n",
            "index is:  628000\n",
            "index is:  629000\n",
            "index is:  630000\n",
            "index is:  631000\n",
            "index is:  632000\n",
            "index is:  633000\n",
            "index is:  634000\n",
            "index is:  635000\n",
            "index is:  636000\n",
            "index is:  637000\n",
            "index is:  638000\n",
            "index is:  639000\n",
            "index is:  640000\n",
            "index is:  641000\n",
            "index is:  642000\n",
            "index is:  643000\n",
            "index is:  644000\n",
            "index is:  645000\n",
            "index is:  646000\n",
            "index is:  647000\n",
            "index is:  648000\n",
            "index is:  649000\n",
            "index is:  650000\n",
            "index is:  651000\n",
            "index is:  652000\n",
            "index is:  653000\n",
            "index is:  654000\n",
            "index is:  655000\n",
            "index is:  656000\n",
            "index is:  657000\n",
            "index is:  658000\n",
            "index is:  659000\n",
            "index is:  660000\n",
            "index is:  661000\n",
            "index is:  662000\n",
            "index is:  663000\n",
            "index is:  664000\n",
            "index is:  665000\n",
            "index is:  666000\n",
            "index is:  667000\n",
            "index is:  668000\n",
            "index is:  669000\n",
            "index is:  670000\n",
            "index is:  671000\n",
            "index is:  672000\n",
            "index is:  673000\n",
            "index is:  674000\n",
            "index is:  675000\n",
            "index is:  676000\n",
            "index is:  677000\n",
            "index is:  678000\n",
            "index is:  679000\n",
            "index is:  680000\n",
            "index is:  681000\n",
            "index is:  682000\n",
            "index is:  683000\n",
            "index is:  684000\n",
            "index is:  685000\n",
            "index is:  686000\n",
            "index is:  687000\n",
            "index is:  688000\n",
            "index is:  689000\n",
            "index is:  690000\n",
            "index is:  691000\n",
            "index is:  692000\n",
            "index is:  693000\n",
            "index is:  694000\n",
            "index is:  695000\n",
            "index is:  696000\n",
            "index is:  697000\n",
            "index is:  698000\n",
            "index is:  699000\n",
            "index is:  700000\n",
            "index is:  701000\n",
            "index is:  702000\n",
            "index is:  703000\n",
            "index is:  704000\n",
            "index is:  705000\n",
            "index is:  706000\n",
            "index is:  707000\n",
            "index is:  708000\n",
            "index is:  709000\n",
            "index is:  710000\n",
            "index is:  711000\n",
            "index is:  712000\n",
            "index is:  713000\n",
            "index is:  714000\n",
            "index is:  715000\n",
            "index is:  716000\n",
            "index is:  717000\n",
            "index is:  718000\n",
            "index is:  719000\n",
            "index is:  720000\n",
            "index is:  721000\n",
            "index is:  722000\n",
            "index is:  723000\n",
            "index is:  724000\n",
            "index is:  725000\n",
            "index is:  726000\n",
            "index is:  727000\n",
            "index is:  728000\n",
            "index is:  729000\n",
            "index is:  730000\n",
            "index is:  731000\n",
            "index is:  732000\n",
            "index is:  733000\n",
            "index is:  734000\n",
            "index is:  735000\n",
            "index is:  736000\n",
            "index is:  737000\n",
            "index is:  738000\n",
            "index is:  739000\n",
            "index is:  740000\n",
            "index is:  741000\n",
            "index is:  742000\n",
            "index is:  743000\n",
            "index is:  744000\n",
            "index is:  745000\n",
            "index is:  746000\n",
            "index is:  747000\n",
            "index is:  748000\n",
            "index is:  749000\n",
            "index is:  750000\n",
            "index is:  751000\n",
            "index is:  752000\n",
            "index is:  753000\n",
            "index is:  754000\n",
            "index is:  755000\n",
            "index is:  756000\n",
            "index is:  757000\n",
            "index is:  758000\n",
            "index is:  759000\n",
            "index is:  760000\n",
            "index is:  761000\n",
            "index is:  762000\n",
            "index is:  763000\n",
            "index is:  764000\n",
            "index is:  765000\n",
            "index is:  766000\n",
            "index is:  767000\n",
            "index is:  768000\n",
            "index is:  769000\n",
            "index is:  770000\n",
            "index is:  771000\n",
            "index is:  772000\n",
            "index is:  773000\n",
            "index is:  774000\n",
            "index is:  775000\n",
            "index is:  776000\n",
            "index is:  777000\n",
            "index is:  778000\n",
            "index is:  779000\n",
            "index is:  780000\n",
            "index is:  781000\n",
            "index is:  782000\n",
            "index is:  783000\n",
            "index is:  784000\n",
            "index is:  785000\n",
            "index is:  786000\n",
            "index is:  787000\n",
            "index is:  788000\n",
            "index is:  789000\n",
            "index is:  790000\n",
            "index is:  791000\n",
            "index is:  792000\n",
            "index is:  793000\n",
            "index is:  794000\n",
            "index is:  795000\n",
            "index is:  796000\n",
            "index is:  797000\n",
            "index is:  798000\n",
            "index is:  799000\n",
            "index is:  800000\n",
            "index is:  801000\n",
            "index is:  802000\n",
            "index is:  803000\n",
            "index is:  804000\n",
            "index is:  805000\n",
            "index is:  806000\n",
            "index is:  807000\n",
            "index is:  808000\n",
            "index is:  809000\n",
            "index is:  810000\n",
            "index is:  811000\n",
            "index is:  812000\n",
            "index is:  813000\n",
            "index is:  814000\n",
            "index is:  815000\n",
            "index is:  816000\n",
            "index is:  817000\n",
            "index is:  818000\n",
            "index is:  819000\n",
            "index is:  820000\n",
            "index is:  821000\n",
            "index is:  822000\n",
            "index is:  823000\n",
            "index is:  824000\n",
            "index is:  825000\n",
            "index is:  826000\n",
            "index is:  827000\n",
            "index is:  828000\n",
            "index is:  829000\n",
            "index is:  830000\n",
            "index is:  831000\n",
            "index is:  832000\n",
            "index is:  833000\n",
            "index is:  834000\n",
            "index is:  835000\n",
            "index is:  836000\n",
            "index is:  837000\n",
            "index is:  838000\n",
            "index is:  839000\n",
            "index is:  840000\n",
            "index is:  841000\n",
            "index is:  842000\n",
            "index is:  843000\n",
            "index is:  844000\n",
            "index is:  845000\n",
            "index is:  846000\n",
            "index is:  847000\n",
            "index is:  848000\n",
            "index is:  849000\n",
            "index is:  850000\n",
            "index is:  851000\n",
            "index is:  852000\n",
            "index is:  853000\n",
            "index is:  854000\n",
            "index is:  855000\n",
            "index is:  856000\n",
            "index is:  857000\n",
            "index is:  858000\n",
            "index is:  859000\n",
            "index is:  860000\n",
            "index is:  861000\n",
            "index is:  862000\n",
            "index is:  863000\n",
            "index is:  864000\n",
            "index is:  865000\n",
            "index is:  866000\n",
            "index is:  867000\n",
            "index is:  868000\n",
            "index is:  869000\n",
            "index is:  870000\n",
            "index is:  871000\n",
            "index is:  872000\n",
            "index is:  873000\n",
            "index is:  874000\n",
            "index is:  875000\n",
            "index is:  876000\n",
            "index is:  877000\n",
            "index is:  878000\n",
            "index is:  879000\n",
            "index is:  880000\n",
            "index is:  881000\n",
            "index is:  882000\n",
            "index is:  883000\n",
            "index is:  884000\n",
            "index is:  885000\n",
            "index is:  886000\n",
            "index is:  887000\n",
            "index is:  888000\n",
            "index is:  889000\n",
            "index is:  890000\n",
            "index is:  891000\n",
            "index is:  892000\n",
            "index is:  893000\n",
            "index is:  894000\n",
            "index is:  895000\n",
            "index is:  896000\n",
            "index is:  897000\n",
            "index is:  898000\n",
            "index is:  899000\n",
            "index is:  900000\n",
            "index is:  901000\n",
            "index is:  902000\n",
            "index is:  903000\n",
            "index is:  904000\n",
            "index is:  905000\n",
            "index is:  906000\n",
            "index is:  907000\n",
            "index is:  908000\n",
            "index is:  909000\n",
            "index is:  910000\n",
            "index is:  911000\n",
            "index is:  912000\n",
            "index is:  913000\n",
            "index is:  914000\n",
            "index is:  915000\n",
            "index is:  916000\n",
            "index is:  917000\n",
            "index is:  918000\n",
            "index is:  919000\n",
            "index is:  920000\n",
            "index is:  921000\n",
            "index is:  922000\n",
            "index is:  923000\n",
            "index is:  924000\n",
            "index is:  925000\n",
            "index is:  926000\n",
            "index is:  927000\n",
            "index is:  928000\n",
            "index is:  929000\n",
            "index is:  930000\n",
            "index is:  931000\n",
            "index is:  932000\n",
            "index is:  933000\n",
            "index is:  934000\n",
            "index is:  935000\n",
            "index is:  936000\n",
            "index is:  937000\n",
            "index is:  938000\n",
            "index is:  939000\n",
            "index is:  940000\n",
            "index is:  941000\n",
            "index is:  942000\n",
            "index is:  943000\n",
            "index is:  944000\n",
            "index is:  945000\n",
            "index is:  946000\n",
            "index is:  947000\n",
            "index is:  948000\n",
            "index is:  949000\n",
            "index is:  950000\n",
            "index is:  951000\n",
            "index is:  952000\n",
            "index is:  953000\n",
            "index is:  954000\n",
            "index is:  955000\n",
            "index is:  956000\n",
            "index is:  957000\n",
            "index is:  958000\n",
            "index is:  959000\n",
            "index is:  960000\n",
            "index is:  961000\n",
            "index is:  962000\n",
            "index is:  963000\n",
            "index is:  964000\n",
            "index is:  965000\n",
            "index is:  966000\n",
            "index is:  967000\n",
            "index is:  968000\n",
            "index is:  969000\n",
            "index is:  970000\n",
            "index is:  971000\n",
            "index is:  972000\n",
            "index is:  973000\n",
            "index is:  974000\n",
            "index is:  975000\n",
            "index is:  976000\n",
            "index is:  977000\n",
            "index is:  978000\n",
            "index is:  979000\n",
            "index is:  980000\n",
            "index is:  981000\n",
            "index is:  982000\n",
            "index is:  983000\n",
            "index is:  984000\n",
            "index is:  985000\n",
            "index is:  986000\n",
            "index is:  987000\n",
            "index is:  988000\n",
            "index is:  989000\n",
            "index is:  990000\n",
            "index is:  991000\n",
            "index is:  992000\n",
            "index is:  993000\n",
            "index is:  994000\n",
            "index is:  995000\n",
            "index is:  996000\n",
            "index is:  997000\n",
            "index is:  998000\n",
            "index is:  999000\n",
            "index is:  1000000\n",
            "FITTING MODEL ON BATCH...\n",
            "DONE CONVERTING\n",
            "Epoch 1/5000\n",
            "1000/1000 [==============================] - 70s 62ms/step - loss: 3.7998 - sparse_categorical_accuracy: 0.2181 - sparse_top_k_categorical_accuracy: 0.4667\n",
            "Epoch 2/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.6950 - sparse_categorical_accuracy: 0.2317 - sparse_top_k_categorical_accuracy: 0.4827\n",
            "Epoch 3/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.6284 - sparse_categorical_accuracy: 0.2394 - sparse_top_k_categorical_accuracy: 0.4926\n",
            "Epoch 4/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.5837 - sparse_categorical_accuracy: 0.2465 - sparse_top_k_categorical_accuracy: 0.4992\n",
            "Epoch 5/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.5475 - sparse_categorical_accuracy: 0.2512 - sparse_top_k_categorical_accuracy: 0.5039\n",
            "Epoch 6/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.5193 - sparse_categorical_accuracy: 0.2551 - sparse_top_k_categorical_accuracy: 0.5075\n",
            "Epoch 7/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.4887 - sparse_categorical_accuracy: 0.2586 - sparse_top_k_categorical_accuracy: 0.5135\n",
            "Epoch 8/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.4700 - sparse_categorical_accuracy: 0.2609 - sparse_top_k_categorical_accuracy: 0.5151\n",
            "Epoch 9/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.4518 - sparse_categorical_accuracy: 0.2639 - sparse_top_k_categorical_accuracy: 0.5191\n",
            "Epoch 10/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.4390 - sparse_categorical_accuracy: 0.2656 - sparse_top_k_categorical_accuracy: 0.5189\n",
            "Epoch 11/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.4127 - sparse_categorical_accuracy: 0.2686 - sparse_top_k_categorical_accuracy: 0.5233\n",
            "Epoch 12/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.4035 - sparse_categorical_accuracy: 0.2707 - sparse_top_k_categorical_accuracy: 0.5254\n",
            "Epoch 13/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.3928 - sparse_categorical_accuracy: 0.2706 - sparse_top_k_categorical_accuracy: 0.5266\n",
            "Epoch 14/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.3811 - sparse_categorical_accuracy: 0.2737 - sparse_top_k_categorical_accuracy: 0.5286\n",
            "Epoch 15/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.3726 - sparse_categorical_accuracy: 0.2739 - sparse_top_k_categorical_accuracy: 0.5287\n",
            "Epoch 16/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.3670 - sparse_categorical_accuracy: 0.2752 - sparse_top_k_categorical_accuracy: 0.5301\n",
            "Epoch 17/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.3542 - sparse_categorical_accuracy: 0.2764 - sparse_top_k_categorical_accuracy: 0.5319\n",
            "Epoch 18/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.3441 - sparse_categorical_accuracy: 0.2789 - sparse_top_k_categorical_accuracy: 0.5341\n",
            "Epoch 19/5000\n",
            "1000/1000 [==============================] - 62s 62ms/step - loss: 3.3357 - sparse_categorical_accuracy: 0.2789 - sparse_top_k_categorical_accuracy: 0.5344\n",
            "Epoch 20/5000\n",
            " 239/1000 [======>.......................] - ETA: 46s - loss: 3.3446 - sparse_categorical_accuracy: 0.2799 - sparse_top_k_categorical_accuracy: 0.5355"
          ]
        }
      ],
      "source": [
        "from keras.saving.saved_model import json_utils\n",
        "mcp_save = ModelCheckpoint('drive/MyDrive/transformer_2.hdf5', save_best_only=True, monitor='val_sparse_categorical_accuracy', mode='max')\n",
        "filename='drive/MyDrive/t_log.csv'\n",
        "history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
        "\n",
        "seq_length = max_input_len\n",
        "\n",
        "dataX = []\n",
        "\n",
        "dataY = []\n",
        "j = 0\n",
        "#pkl = pkl.sample(frac=1)\n",
        "\n",
        "pred_len = max_input_len\n",
        "\n",
        "# for iteration in range(5000):\n",
        "\n",
        "# pkl = pkl.sample(frac=.1)\n",
        "\n",
        "dataX = []\n",
        "target = []\n",
        "dataY = []\n",
        "for idx, row in pkl.iterrows():\n",
        "  j += 1\n",
        "\n",
        "  cur_seq = pkl.loc[idx][\"encoded\"]\n",
        "  cur_seq = list(filter(lambda x: x > 2, cur_seq))\n",
        "  #print(cur_seq)\n",
        "  #tokenizer.fit_on_texts(cur_seq)\n",
        "  # cur_seq = [tokenizer.word_index[word.strip()] for word in cur_seq]\n",
        "  cur_seq = [0]*seq_length + [dictionary_dim - 1] + cur_seq + [dictionary_dim]\n",
        "  for i in range(len(cur_seq) - seq_length - pred_len):\n",
        "    cur = cur_seq[i:i+seq_length]\n",
        "    cur = [0]*(max_input_len - len(cur)) + cur\n",
        "    #print(len(cur))\n",
        "    dataX.append(cur)\n",
        "    #print(type(cur_seq[i+seq_length]))\n",
        "    #print([[y] for y in cur_seq[i+seq_length: i+seq_length+pred_len]])\n",
        "    # target.append([dictionary_dim-1]*1 + cur_seq[i+seq_length: i+seq_length+pred_len-1])\n",
        "    dataY.append(cur_seq[i+seq_length+pred_len-1])\n",
        "  if j % 1000 == 0:\n",
        "    print(\"index is: \", j)\n",
        "\n",
        "  if j % 1000000 == 0:\n",
        "    print(\"FITTING MODEL ON BATCH...\")\n",
        "\n",
        "    x_t = np.array(dataX)\n",
        "    y_t = np.array(dataY)\n",
        "\n",
        "    print(\"DONE CONVERTING\")\n",
        "\n",
        "    \n",
        "\n",
        "    # for num in range(1):\n",
        "    #   idx = np.random.randint(0, dataX.shape[0], 10000)\n",
        "    #   x_sample = dataX[idx]\n",
        "    #   y_sample = dataY[idx]\n",
        "    #   # target_sample = target[idx]\n",
        "\n",
        "    #   # zeros = np.zeros((np.shape(dataY)[0],dictionary_dim))\n",
        "\n",
        "    #   # for i in range(dictionary_dim):\n",
        "    #   #   print(zeros.shape)\n",
        "    #   #   zeros[dataY[i],i] = 1\n",
        "\n",
        "    #   # dataY = zeros\n",
        "\n",
        "    #   print(x_sample.shape)\n",
        "\n",
        "    model.fit(x_t, y_t, epochs=5000, batch_size=512, steps_per_epoch = 1000, verbose=1, shuffle=True)\n",
        "\n",
        "    # x_sample = []\n",
        "    # y_sample = []\n",
        "    #   # target_sample = []\n",
        "\n",
        "    #   # print(dataX.shape)\n",
        "    #   # print(target.shape)\n",
        "    #   # print(dataY.shape)\n",
        "\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    # target = []\n",
        "\n",
        "    #seq_length = random.randint(4,max_input_len)\n",
        "\n",
        "    # if j % 100000 == 0:  \n",
        "    #   model.save(\"drive/MyDrive/samples/char_transformer.h5\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eClXFxZrPhAc",
        "outputId": "82452cd1-0cec-47d6-b0cf-183a490dd182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "29249/60349 [=============>................] - ETA: 31:50 - loss: 3.1468 - sparse_categorical_accuracy: 0.3051 - sparse_top_k_categorical_accuracy: 0.5636"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_t, y_t, epochs=100, batch_size=512, verbose=1, \n",
        "                callbacks=[mcp_save, history_logger], shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HjeWjjswEi_",
        "outputId": "ba2e6eff-1198-4dd7-d3d3-7d73ee7cad0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fact_list\n",
            "['if', 'x', '==', 'y', '', '']\n",
            "(8,)\n",
            "(1, 8)\n",
            "[[ 0  0  0  0 20 27 25 49]]\n",
            "[]\n",
            "[0.16272257]\n",
            "2\n",
            "[2]\n",
            "[ 4  1  2 49 27]\n",
            "['(', '<UNK>', '\\n', 'y', 'x']\n"
          ]
        }
      ],
      "source": [
        "1import tokenize\n",
        "import io\n",
        "\n",
        "test_sample = \"\"\"if x == y\"\"\"\n",
        "\n",
        "print(tokenizer.index_word[9998])\n",
        "\n",
        "# target = np.array([9998] + [0]*(max_input_len - 1))\n",
        "\n",
        "buf = io.StringIO(test_sample)\n",
        "\n",
        "tokens = [tok.string.strip() for tok in tokenize.generate_tokens(buf.readline)]\n",
        "print(tokens)\n",
        "\n",
        "seq = tokenizer.texts_to_sequences(tokens)\n",
        "inp = []\n",
        "\n",
        "for x in seq:\n",
        "  if len(x) != 0:\n",
        "    inp.append(x[0])\n",
        "\n",
        "if len(inp) > max_input_len:\n",
        "  inp = inp[-10:-1]\n",
        "  print(inp)\n",
        "\n",
        "seq = np.array([0]*(max_input_len - len(inp)) + inp)\n",
        "print(seq.shape)\n",
        "seq = seq.reshape((1, seq.shape[0]))\n",
        "# target = target.reshape((1, target.shape[0]))\n",
        "print(seq.shape)\n",
        "print(seq)\n",
        "print(target)\n",
        "\n",
        "pred = model.predict([seq, target])\n",
        "pred = pred.flatten()\n",
        "\n",
        "max = np.argmax(pred)\n",
        "ind = np.argpartition(pred, -1)[-1:]\n",
        "\n",
        "\n",
        "print(pred[ind])\n",
        "print(max)\n",
        "print(ind)\n",
        "\n",
        "max = np.argmax(pred)\n",
        "ind = np.argpartition(pred, -5)[-5:]\n",
        "print(ind)\n",
        "# idx = [[int(np.argmax(pred, axis=1))]]\n",
        "# print(idx)\n",
        "\n",
        "print(tokenizer.sequences_to_texts([[x] for x in ind]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orGWhZgFPNRk",
        "outputId": "370ef5a6-dcb1-4c90-940f-9b37cb515f50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_8 (InputLayer)           [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_10 (Embedding)       (None, 10, 100)      1000000     ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_22 (TFOpL  (None, 10, 100)     0           ['embedding_10[0][0]']           \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " input_7 (InputLayer)           [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " tf.compat.v1.shape_18 (TFOpLam  (3,)                0           ['tf.__operators__.add_22[0][0]']\n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.compat.v1.shape_19 (TFOpLam  (3,)                0           ['tf.__operators__.add_22[0][0]']\n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " embedding_9 (Embedding)        (None, 10, 100)      1000000     ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_116 (  ()                  0           ['tf.compat.v1.shape_18[0][0]']  \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_122 (  ()                  0           ['tf.compat.v1.shape_19[0][0]']  \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.add_21 (TFOpL  (None, 10, 100)     0           ['embedding_9[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.range_36 (TFOpLambda)       (10,)                0           ['tf.__operators__.getitem_116[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " tf.range_38 (TFOpLambda)       (10,)                0           ['tf.__operators__.getitem_122[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " multi_head_attention_9 (MultiH  (None, 10, 100)     2579300     ['tf.__operators__.add_21[0][0]',\n",
            " eadAttention)                                                    'tf.__operators__.add_21[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_117 (  (10, 1)             0           ['tf.range_36[0][0]']            \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.range_37 (TFOpLambda)       (10,)                0           ['tf.__operators__.getitem_116[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_123 (  (10, 1)             0           ['tf.range_38[0][0]']            \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.range_39 (TFOpLambda)       (10,)                0           ['tf.__operators__.getitem_122[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 10, 100)      0           ['multi_head_attention_9[0][0]'] \n",
            "                                                                                                  \n",
            " tf.math.greater_equal_18 (TFOp  (10, 10)            0           ['tf.__operators__.getitem_117[0]\n",
            " Lambda)                                                         [0]',                            \n",
            "                                                                  'tf.range_37[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_115 (  ()                  0           ['tf.compat.v1.shape_18[0][0]']  \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.math.greater_equal_19 (TFOp  (10, 10)            0           ['tf.__operators__.getitem_123[0]\n",
            " Lambda)                                                         [0]',                            \n",
            "                                                                  'tf.range_39[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_23 (TFOpL  (None, 10, 100)     0           ['tf.__operators__.add_21[0][0]',\n",
            " ambda)                                                           'dropout_12[0][0]']             \n",
            "                                                                                                  \n",
            " tf.cast_36 (TFOpLambda)        (10, 10)             0           ['tf.math.greater_equal_18[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_118 (  ()                  0           ['tf.compat.v1.shape_18[0][0]']  \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_119 (  ()                  0           ['tf.compat.v1.shape_18[0][0]']  \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.expand_dims_18 (TFOpLambda)  (1,)                0           ['tf.__operators__.getitem_115[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " tf.cast_38 (TFOpLambda)        (10, 10)             0           ['tf.math.greater_equal_19[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_124 (  ()                  0           ['tf.compat.v1.shape_19[0][0]']  \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_125 (  ()                  0           ['tf.compat.v1.shape_19[0][0]']  \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_121 (  ()                  0           ['tf.compat.v1.shape_19[0][0]']  \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " layer_normalization_15 (LayerN  (None, 10, 100)     200         ['tf.__operators__.add_23[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.reshape_18 (TFOpLambda)     (1, 10, 10)          0           ['tf.cast_36[0][0]',             \n",
            "                                                                  'tf.__operators__.getitem_118[0]\n",
            "                                                                 [0]',                            \n",
            "                                                                  'tf.__operators__.getitem_119[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " tf.concat_18 (TFOpLambda)      (3,)                 0           ['tf.expand_dims_18[0][0]']      \n",
            "                                                                                                  \n",
            " tf.reshape_19 (TFOpLambda)     (1, 10, 10)          0           ['tf.cast_38[0][0]',             \n",
            "                                                                  'tf.__operators__.getitem_124[0]\n",
            "                                                                 [0]',                            \n",
            "                                                                  'tf.__operators__.getitem_125[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " tf.expand_dims_19 (TFOpLambda)  (1,)                0           ['tf.__operators__.getitem_121[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 10, 512)      51712       ['layer_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " tf.tile_37 (TFOpLambda)        (None, 10, 10)       0           ['tf.reshape_18[0][0]',          \n",
            "                                                                  'tf.concat_18[0][0]']           \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_126 (  (1, 1, 10, 10)      0           ['tf.reshape_19[0][0]']          \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.concat_19 (TFOpLambda)      (3,)                 0           ['tf.expand_dims_19[0][0]']      \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 10, 100)      51300       ['dense_18[0][0]']               \n",
            "                                                                                                  \n",
            " multi_head_attention_10 (Multi  (None, 10, 100)     2579300     ['tf.__operators__.add_22[0][0]',\n",
            " HeadAttention)                                                   'tf.tile_37[0][0]',             \n",
            "                                                                  'tf.__operators__.add_22[0][0]',\n",
            "                                                                  'tf.__operators__.add_22[0][0]']\n",
            "                                                                                                  \n",
            " tf.cast_39 (TFOpLambda)        (1, 1, 10, 10)       0           ['tf.__operators__.getitem_126[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " tf.tile_38 (TFOpLambda)        (None, 10, 10)       0           ['tf.reshape_19[0][0]',          \n",
            "                                                                  'tf.concat_19[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 10, 100)      0           ['dense_19[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_25 (TFOpL  (None, 10, 100)     0           ['tf.__operators__.add_22[0][0]',\n",
            " ambda)                                                           'multi_head_attention_10[0][0]']\n",
            "                                                                                                  \n",
            " tf.math.minimum_19 (TFOpLambda  (1, None, 10, 10)   0           ['tf.cast_39[0][0]',             \n",
            " )                                                                'tf.tile_38[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_24 (TFOpL  (None, 10, 100)     0           ['layer_normalization_15[0][0]', \n",
            " ambda)                                                           'dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_17 (LayerN  (None, 10, 100)     200         ['tf.__operators__.add_25[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_127 (  (None, 10, 10)      0           ['tf.math.minimum_19[0][0]']     \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " layer_normalization_16 (LayerN  (None, 10, 100)     200         ['tf.__operators__.add_24[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_11 (Multi  (None, 10, 100)     2579300     ['layer_normalization_17[0][0]', \n",
            " HeadAttention)                                                   'tf.__operators__.getitem_127[0]\n",
            "                                                                 [0]',                            \n",
            "                                                                  'layer_normalization_16[0][0]', \n",
            "                                                                  'layer_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_26 (TFOpL  (None, 10, 100)     0           ['layer_normalization_17[0][0]', \n",
            " ambda)                                                           'multi_head_attention_11[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_18 (LayerN  (None, 10, 100)     200         ['tf.__operators__.add_26[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 10, 512)      51712       ['layer_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (None, 10, 100)      51300       ['dense_20[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 10, 100)      0           ['dense_21[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_27 (TFOpL  (None, 10, 100)     0           ['layer_normalization_18[0][0]', \n",
            " ambda)                                                           'dropout_14[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_19 (LayerN  (None, 10, 100)     200         ['tf.__operators__.add_27[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 10, 100)      0           ['layer_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 1000)         0           ['dropout_15[0][0]']             \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 5000)         5005000     ['flatten_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 10000)        50010000    ['dense_22[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 64,959,924\n",
            "Trainable params: 64,959,924\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model=load_model(\"drive/MyDrive/samples/transformer_model_3.h5\")\n",
        "model.summary()\n",
        "\n",
        "max_input_len = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soR7On9az9T2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Copy_of_Aaron's_Full_Transformer_Implementation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}